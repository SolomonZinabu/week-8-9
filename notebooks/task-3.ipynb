{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP values structure: <class 'numpy.ndarray'> 100 (10, 2)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The shape of the shap_values matrix does not match the shape of the provided data matrix.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSHAP values structure:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(shap_values), \u001b[38;5;28mlen\u001b[39m(shap_values), shap_values[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m### SHAP Summary Plot\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Use shap_values[1] if you are focusing on the fraud class (1)\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m### SHAP Force Plot\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Choose an instance to explain\u001b[39;00m\n\u001b[0;32m     42\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "File \u001b[1;32md:\\Projects\\SolomonProjects\\Kifiya\\week 8&9\\week-8-9\\venv\\Lib\\site-packages\\shap\\plots\\_beeswarm.py:543\u001b[0m, in \u001b[0;36msummary_legacy\u001b[1;34m(shap_values, features, feature_names, max_display, plot_type, color, axis_color, title, alpha, show, sort, color_bar, plot_size, layered_violin_max_num_bins, class_names, class_inds, color_bar_label, cmap, show_values_in_legend, use_log_scale)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, shape_msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Perhaps the extra column in the shap_values matrix is the \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m    541\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant offset? Of so just pass shap_values[:,:-1].\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 543\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m num_features \u001b[38;5;241m==\u001b[39m features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], shape_msg\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    546\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFEATURE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_features)])\n",
      "\u001b[1;31mAssertionError\u001b[0m: The shape of the shap_values matrix does not match the shape of the provided data matrix."
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the preprocessed fraud data and the trained Random Forest model\n",
    "fraud_data = pd.read_csv('../data/Fraud_Data.csv')\n",
    "model = joblib.load('../models/random_forest_fraud_model.pkl')\n",
    "\n",
    "# Drop non-numeric or irrelevant columns\n",
    "fraud_data_encoded = fraud_data.drop(columns=['signup_time', 'purchase_time', 'device_id', 'user_id'])\n",
    "\n",
    "# Convert categorical variables to one-hot encoding (must match training data format)\n",
    "fraud_data_encoded = pd.get_dummies(fraud_data_encoded, columns=['source', 'browser', 'sex'], drop_first=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = fraud_data_encoded.drop(columns=['class'])\n",
    "y = fraud_data_encoded['class']\n",
    "\n",
    "# Ensure SHAP can handle the data type\n",
    "X_sample = X.sample(100)  # SHAP handles smaller samples for faster computation\n",
    "\n",
    "# 1. SHAP Model Explainability\n",
    "## Initialize the SHAP explainer with the Random Forest model\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "## Generate SHAP values with additivity check disabled\n",
    "shap_values = explainer.shap_values(X_sample, check_additivity=False)\n",
    "\n",
    "# Inspect the structure of shap_values to ensure compatibility\n",
    "print(\"SHAP values structure:\", type(shap_values), len(shap_values), shap_values[1].shape)\n",
    "\n",
    "### SHAP Summary Plot\n",
    "# Use shap_values[1] if you are focusing on the fraud class (1)\n",
    "shap.summary_plot(shap_values[1], X_sample, plot_type=\"bar\", show=True)\n",
    "\n",
    "### SHAP Force Plot\n",
    "# Choose an instance to explain\n",
    "index = 10\n",
    "shap.initjs()  # Load JS for visualizations\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][index], X_sample.iloc[index], matplotlib=True)\n",
    "\n",
    "### SHAP Dependence Plot\n",
    "# Select a feature to see its dependence with the prediction\n",
    "shap.dependence_plot(\"purchase_value\", shap_values[1], X_sample)\n",
    "\n",
    "# 2. LIME Model Explainability\n",
    "## Initialize LIME explainer\n",
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(X.values, \n",
    "                                                        feature_names=X.columns,\n",
    "                                                        class_names=['Not Fraud', 'Fraud'],\n",
    "                                                        mode='classification')\n",
    "\n",
    "## Choose an instance to explain\n",
    "instance_index = 10\n",
    "instance = X.iloc[instance_index].values.reshape(1, -1)\n",
    "lime_exp = lime_explainer.explain_instance(X.iloc[instance_index].values, model.predict_proba)\n",
    "\n",
    "### LIME Explanation Plot\n",
    "lime_exp.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\SolomonProjects\\Kifiya\\week 8&9\\week-8-9\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ExplainerError",
     "evalue": "Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. Consider retrying with the feature_perturbation='interventional' option. This check failed because for one of the samples the sum of the SHAP values was 0.877653, while the model output was 0.900000. If this difference is acceptable you can set check_additivity=False to disable this check.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExplainerError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mTreeExplainer(model)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m## Generate SHAP values\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m### SHAP Summary Plot\u001b[39;00m\n\u001b[0;32m     34\u001b[0m shap\u001b[38;5;241m.\u001b[39msummary_plot(shap_values[\u001b[38;5;241m1\u001b[39m], X_sample, plot_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m\"\u001b[39m, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Projects\\SolomonProjects\\Kifiya\\week 8&9\\week-8-9\\venv\\Lib\\site-packages\\shap\\explainers\\_tree.py:510\u001b[0m, in \u001b[0;36mTreeExplainer.shap_values\u001b[1;34m(self, X, y, tree_limit, approximate, check_additivity, from_call)\u001b[0m\n\u001b[0;32m    508\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_shap_output(phi, flat_output)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_additivity \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel_output \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 510\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_additivity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# This statements handles the case of multiple outputs\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m# e.g. a multi-class classification problem, multi-target regression problem\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;66;03m# in this case the output shape corresponds to [num_samples, num_features, num_outputs]\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32md:\\Projects\\SolomonProjects\\Kifiya\\week 8&9\\week-8-9\\venv\\Lib\\site-packages\\shap\\explainers\\_tree.py:678\u001b[0m, in \u001b[0;36mTreeExplainer.assert_additivity\u001b[1;34m(self, phi, model_output)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(phi, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(phi)):\n\u001b[1;32m--> 678\u001b[0m         \u001b[43mcheck_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpected_value\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    680\u001b[0m     check_sum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_value \u001b[38;5;241m+\u001b[39m phi\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), model_output)\n",
      "File \u001b[1;32md:\\Projects\\SolomonProjects\\Kifiya\\week 8&9\\week-8-9\\venv\\Lib\\site-packages\\shap\\explainers\\_tree.py:674\u001b[0m, in \u001b[0;36mTreeExplainer.assert_additivity.<locals>.check_sum\u001b[1;34m(sum_val, model_output)\u001b[0m\n\u001b[0;32m    670\u001b[0m     err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Consider retrying with the feature_perturbation=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterventional\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m option.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    671\u001b[0m err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m This check failed because for one of the samples the sum of the SHAP values\u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m    672\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m was \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msum_val[ind]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124mf\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, while the model output was \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_output[ind]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124mf\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If this\u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m    673\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m difference is acceptable you can set check_additivity=False to disable this check.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 674\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ExplainerError(err_msg)\n",
      "\u001b[1;31mExplainerError\u001b[0m: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. Consider retrying with the feature_perturbation='interventional' option. This check failed because for one of the samples the sum of the SHAP values was 0.877653, while the model output was 0.900000. If this difference is acceptable you can set check_additivity=False to disable this check."
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the preprocessed fraud data and the trained Random Forest model\n",
    "fraud_data = pd.read_csv('../data/Fraud_Data.csv')\n",
    "model = joblib.load('../models/random_forest_fraud_model.pkl')\n",
    "\n",
    "# Drop non-numeric or irrelevant columns\n",
    "fraud_data_encoded = fraud_data.drop(columns=['signup_time', 'purchase_time', 'device_id', 'user_id'])\n",
    "\n",
    "# Convert categorical variables to one-hot encoding (must match training data format)\n",
    "fraud_data_encoded = pd.get_dummies(fraud_data_encoded, columns=['source', 'browser', 'sex'], drop_first=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = fraud_data_encoded.drop(columns=['class'])\n",
    "y = fraud_data_encoded['class']\n",
    "\n",
    "# Ensure SHAP can handle the data type\n",
    "X_sample = X.sample(100)  # SHAP handles smaller samples for faster computation\n",
    "\n",
    "# 1. SHAP Model Explainability\n",
    "## Initialize the SHAP explainer with the Random Forest model\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "## Generate SHAP values\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "### SHAP Summary Plot\n",
    "shap.summary_plot(shap_values[1], X_sample, plot_type=\"bar\", show=True)\n",
    "\n",
    "### SHAP Force Plot\n",
    "# Choose an instance to explain\n",
    "index = 10\n",
    "shap.initjs()  # Load JS for visualizations\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][index], X_sample.iloc[index], matplotlib=True)\n",
    "\n",
    "### SHAP Dependence Plot\n",
    "# Select a feature to see its dependence with the prediction\n",
    "shap.dependence_plot(\"purchase_value\", shap_values[1], X_sample)\n",
    "\n",
    "# 2. LIME Model Explainability\n",
    "## Initialize LIME explainer\n",
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(X.values, \n",
    "                                                        feature_names=X.columns,\n",
    "                                                        class_names=['Not Fraud', 'Fraud'],\n",
    "                                                        mode='classification')\n",
    "\n",
    "## Choose an instance to explain\n",
    "instance_index = 10\n",
    "instance = X.iloc[instance_index].values.reshape(1, -1)\n",
    "lime_exp = lime_explainer.explain_instance(X.iloc[instance_index].values, model.predict_proba)\n",
    "\n",
    "### LIME Explanation Plot\n",
    "lime_exp.show_in_notebook(show_table=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
